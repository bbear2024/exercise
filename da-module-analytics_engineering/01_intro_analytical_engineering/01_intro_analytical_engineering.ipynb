{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44535db6",
   "metadata": {},
   "source": [
    "#### Week 7 - Analytical Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f104eb",
   "metadata": {},
   "source": [
    "# Lesson 1: Intro to Analytical Engineering / Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69158a66",
   "metadata": {},
   "source": [
    "> ### Warm up exercise:\n",
    ">\n",
    "> To start the second week of working with databases we want to come back to the data protection topic. >This warm up exercise is meant to prepare the students for how to work with credentials this week.\n",
    "> Split the students into 3 groups and assign every group with one question:\n",
    ">\n",
    "> - what is a bad password? (present some concrete ideas)\n",
    ">   - the same password everywhere\n",
    ">    - set of consecutive numbers like 123456...\n",
    ">    - string 'password', 'qwerty', etc.\n",
    ">    - short password\n",
    ">    - only (lowercase) letters\n",
    ">    - including your name or any other personal data in the password\n",
    ">    - using real words\n",
    ">\n",
    "> - how to not store a password?\n",
    ">   - on a piece of paper\n",
    ">   - or in general in a place easily visible by others\n",
    ">    - github\n",
    ">    - shared file\n",
    ">    - sent as a message to someone \n",
    ">\n",
    "> - what might go wrong when experiencing a data breach?\n",
    "> - identity stealing\n",
    ">   - money stealing\n",
    ">    - personal data leakage...\n",
    "\n",
    "After the group work, ask the students to present their ideas.\n",
    "At the end mention that we will try to make storing credentials more secure this week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258a5b7",
   "metadata": {},
   "source": [
    "Present the project of the week:\n",
    "\n",
    "##### ðŸŽ¯ Project \"Weather vs Flights Data\" Goal: Construct ELT data pipeline using python, pandas, more advanced SQL, SQLalchemy, and dbt\n",
    "\n",
    "In this week, you will learn how to connect to your database with Python and how to design and to automate a pipeline. Next week we could use the collected data for a Dashboard to visualize the results. \n",
    "\n",
    "The next days, you will work with a comprehensive real world API from https://dev.meteostat.net/api/. We will get access to historical weather records (daily and hourly) from basically everywhere around over the world. We will aim to obtain weather data for 3 airport locations: New York (JFK), Los Angeles (LAX), And Miami (MIA) from the last year. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c03a3",
   "metadata": {},
   "source": [
    "### Weather Pipeline Steps : \n",
    "https://docs.google.com/presentation/d/1nzs5R1U-dkHu19YMmKkSLXKLSHYm5XozARAx34SzYJw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de3e45",
   "metadata": {},
   "source": [
    "#### Milestones\n",
    "\n",
    "1. Access data from real world API using a python script\n",
    "\n",
    "2. Using SQLalchemy import raw data into postgres database \n",
    "\n",
    "3. (optional: we don't have it yet) Setting logging in order to catch problems and log successes\n",
    "\n",
    "4. Connect database to dbt cloud - which will be used to parse and prepare the data\n",
    "\n",
    "5. Learn how to use Common Table Expressions in SQL\n",
    "\n",
    "6. Clean prepare the data on dbt cloud using CTE and the power of dbt\n",
    "\n",
    "7. Answer questions with the data and save the answers in a data mart for your stakeholders\n",
    "\n",
    "   \n",
    "\n",
    "**Note:** Lectures Logging, Managing PostgreSQL users and roles, Query optimization and indexing would go beyond the scope of this week. Consider these topics as the next steps to look into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4beb822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8396c212",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "1. Pyramid of needs in data aka data roles\n",
    "2. Data roles\n",
    "3. Tech stack\n",
    "4. ETL vs ELT\n",
    "5. dbt\n",
    "6. `.env` Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87c1fd",
   "metadata": {},
   "source": [
    "### 1. Pyramid of needs in data aka data roles\n",
    "\n",
    "In 2017 Monica Rogati wrote an [article about pyramid of needs in Data Science](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007). \n",
    "\n",
    "![pyramid](monica_rogati.jpeg)\n",
    "\n",
    "The pyramid of needs stresses the fact that the most essential thing in data projects is data maintenance that includes data collection, data cleaning, data storage, data modelling, etc. which is typically considered data engineering. The next step refers to data analytics and data science / AI should be only a cherry on the top but only if all the other parts of the pyramid were covered before.\n",
    "\n",
    "Even though the article is focused on AI and Data Science it opened a discussion about different parts of data projects, different roles in data and how they are all connected with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883bd99",
   "metadata": {},
   "source": [
    "### 2. Data roles\n",
    "\n",
    "Typically in data we distinguish following roles that will be connected with mentioned tasks and areas:\n",
    "\n",
    "- Data Engineers\n",
    "  - building and maintaining data infrastructure (data pipelines)\n",
    "  - collecting raw data sources \n",
    "  - cleaning and modelling data for the later data analysis usage \n",
    "- Data Analysts\n",
    "  - (exploratory) data analysis\n",
    "  - A/B testing \n",
    "  - communication with stakeholders (domain knowledge)\n",
    "- Data Scientists\n",
    "  - feature engineering\n",
    "  - machine learning\n",
    "  - predictions and estimations\n",
    "\n",
    "However, even though we usually put boundaries between these roles, in reality it's much more complex. Many times data analysts will be covering some of the topics from the data engineering or data science area, and the other way around. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa66d7",
   "metadata": {},
   "source": [
    "### 3. Tech stack\n",
    "\n",
    "There are plenty of tools and types of tools used in data projects. Some of them, as BI tools (e.g. Tableau) are usually exclusive for data analytics, on the other hand many others will be shared between different data roles. \n",
    "\n",
    "![stack](data_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e473d0",
   "metadata": {},
   "source": [
    "### 4. ETL vs ELT\n",
    "\n",
    "> Explain the steps and then the differences between the two types of pipelines. Here is a great resource:\n",
    ">\n",
    "> https://www.geeksforgeeks.org/difference-between-elt-and-etl/\n",
    "\n",
    "Data engineering is many times connected with creating data pipelines - a technical solution that will be responsible for data collection, storing, cleaning and modeling the data for further usage. There are two different strategies or types of data pipelines: ETL and ELT:\n",
    "\n",
    "|       ETL       |       ELT       |\n",
    "| :-------------: | :-------------: |\n",
    "| ![etl](etl.png) | ![elt](elt.png) |\n",
    "\n",
    "The letters in both names refer to:\n",
    "\n",
    "- (E)xtract - identifying data from one or more sources (such as databases, files, ERP, CRM, etc.)\n",
    "- (T)ransform - process of transforming the raw data source into the target format required for analysis projects\n",
    "- (L)oad - storing the extracted raw data (usually in a data warehouse or a data lake)\n",
    "\n",
    "Historically data pipelines used to be ETL ones since it was expensive to store huge amount of data. In ETL structure the transformation and modelling of the data for the further analysis was done before loading (storing it).\n",
    "As we do have more and more easy and cheap ways of storing data, the ELT model became way more popular recently. In this structure data is firstly loaded and could be transformed later on. That means that you don't neccessarily need to know in advance how you want to use your data which makes it more flexible. This model also made it more accessible for data analysts to work on data transformations together with data engineers.\n",
    "\n",
    "In our case our ELT process will be covered by:\n",
    "\n",
    "- (E)xtract - fetching our data (in a json form) using weather API\n",
    "- (L)oad - using a db client (`sqlalchemy`) to connect with a db and sending the raw data to it\n",
    "- (T)tansform - `dbt` as a data transformation tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929fe10c",
   "metadata": {},
   "source": [
    "### 5. What is dbt?\n",
    "\n",
    "**dbt**  is a transformation workflow (it handles the `T` part from ELT) that we're gonna learn about this week. More on that in the following encounters but some of the properties of it are:\n",
    "\n",
    "- **dbt** compiles and runs your analytics code against your data platform\n",
    "- reusable, or modular, data models that can be referenced in subsequent work \n",
    "- you can design tests to check the quality of your data as well as generating documentation for your tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64f6e4",
   "metadata": {},
   "source": [
    "### 6. `.env` Files\n",
    "\n",
    "\n",
    " It's common for teams to maintain distinct \"environments\" for their codebase. These separate environments allow thorough testing before deploying changes to the production environment, where they interact with end-users. In scenarios involving multiple environments, developers often opt to use multiple .env files to store credentials. For instance, they might have one \n",
    " .env file containing database keys for development and another for production.\n",
    "\n",
    " This separation of code and credentials lower the risk of unauthorized individuals gaining access to sensitive data in the cloud.\n",
    "\n",
    "**.env** files are specifically designed to store credentials in a key-value format for the various services that the program utilizes. These files are intended to be stored locally and not shared in online code repositories, ensuring that sensitive information remains confidential. Each developer within a team typically manages one or more `.env` files, tailored for the specific environments they are working on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25f6a3",
   "metadata": {},
   "source": [
    "#### Usage\n",
    "\n",
    "In this section, weâ€™ll walk through how to use a `.env` file in a basic python project.\n",
    "\n",
    "1. To begin, head to the root of your **week folder** and create an empty `.env` file containing credentials youâ€™d like injected into your codebase. It may look something like this:\n",
    "\n",
    "```python\n",
    "POSTGRES_USER = 'marylinmonroe'\n",
    "POSTGRES_PASS = '382jLK393fk3k2' # never add your password to a jupyter notebook!\n",
    "POSTGRES_HOST = 'data-analytics-course-2.c8g8r1deus2v.eu-central-1.rds.amazonaws.com'\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_DB = 'minty_floats'\n",
    "POSTGRES_SCHEMA = 's_marylinmonroe'\n",
    "```\n",
    "\n",
    "2. Keep in mind that the `.env` file should NOT be uploaded to **github**. A file called `.env_example` could be uploaded in order to give an example of what the `.env` file should contain. Therefore the `.env` file should be always in your `.gitignore` file! (it should be there already)\n",
    "\n",
    "\n",
    "3. Now to inject the secrets into your project, you can use a popular module like dotenv; it will parse the `.env` file and make your secrets accessible within your codebase under the process object. Go ahead and install the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fafdd5",
   "metadata": {},
   "source": [
    "4. Import the module at the top of the start script for your codebase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d91be",
   "metadata": {},
   "source": [
    "> **Teacher's Note:**  \n",
    "> there are two ways, first is **less intrusve**, but decide whether you want to introduce both or just one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5a7e1",
   "metadata": {},
   "source": [
    "**First option:** `dotenv_values()` will only read the `.env` file and return a *temporary* dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting API and DB credentials\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values()\n",
    "pg_user = config['POSTGRES_USER']  # align the key label with your .env file !\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']\n",
    "pg_pass = config['POSTGRES_PASS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9045fb",
   "metadata": {},
   "source": [
    "**Second option:** `load_dotenv()` : will actually load the key/values into your running os enviromental variabls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de38836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting API and DB credentials\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "# pg_host = os.getenv('POSTGRES_HOST') # align the key label with your .env file !\n",
    "# pg_user = os.getenv('POSTGRES_USER')  # align the key label with your .env file !\n",
    "# pg_host = os.getenv('POSTGRES_HOST')\n",
    "# pg_port = os.getenv('POSTGRES_PORT')\n",
    "# pg_db = os.getenv('POSTGRES_DB')\n",
    "# pg_schema = os.getenv('POSTGRES_SCHEMA')\n",
    "# pg_pass = os.getenv('POSTGRES_PASS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6084b5a",
   "metadata": {},
   "source": [
    "Cool. Weâ€™ve successfully added a `.env` file into your project with some secrets and accessed those secrets in your codebase. Additionally, when you push your code via git, your secrets will stay on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b179c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
