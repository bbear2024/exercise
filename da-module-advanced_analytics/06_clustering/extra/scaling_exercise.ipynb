{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0b59dd",
   "metadata": {},
   "source": [
    "# Introduction to Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b0f11",
   "metadata": {},
   "source": [
    "Before you implement your first Clustering algorithm by yourself, this notebook teaches you the concept of scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2361db3",
   "metadata": {},
   "source": [
    "## Features Scaling\n",
    "\n",
    "Often the input features of your model have different units which means that the variables also have different scales. While some model types (e.g. tree-based models like decision tree or random forest) are unaffected by the scale of numerical input variables, many machine learning algorithms including f.e. algorithms using distance measures (e.g. K-Means) perform better when the input features are scaled to a specific range.  \n",
    "\n",
    "Since K-Means uses distance measures, we are covering this topic here. \n",
    "\n",
    "The most popular techniques for scaling are **normalization** and **standardization**. \n",
    "\n",
    "Check the [link](https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/) for further info. \n",
    "\n",
    "![](images/normalization_vs_standardization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a457e",
   "metadata": {},
   "source": [
    "**As an example, we will use the cars dataset out of our Linear Regression repo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a754949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b755a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "cars = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n",
    "cars = cars.rename(columns={'Unnamed: 0':'car_model',\n",
    "                            'wt':\"weight\"});\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3858e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we have a look at the different scaling methods, we have to define which columns we want to scale.\n",
    "# Within the linear regression repo, we used weight and horsepower to predict mpg using a multiple linear regression.\n",
    "# Thus, we want to scale the independent variables weight and horsepower.\n",
    "col_scale = ['hp', 'weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bfb5bd",
   "metadata": {},
   "source": [
    "### Data Standardization \n",
    "\n",
    "In order to standardize a dataset it is necessary to rescale the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.  \n",
    "You can think of it as subtracting the mean value or centering the data. \n",
    "Sklearn provides us for this case with the [Standard scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "A value is standardized as follows: \n",
    "\n",
    "$ x_{scaled} = \\frac{x – \\mu}{\\sigma}  $, where \n",
    "\n",
    "$ \\mu = \\frac{\\sum{x}}{m} $ is the mean, where m is the number of observations\n",
    "\n",
    "$ \\sigma = \\sqrt{ \\frac{\\sum{ (x – \\mu)^2 }}{m}} $ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with standard scaler\n",
    "# First, a StandardScaler instance is defined with default hyperparameters.\n",
    "# After defining we can call the fit_transform() function and pass it to our data we want to transform.\n",
    "scaler = StandardScaler()\n",
    "cars_scaled = scaler.fit_transform(cars[col_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result is a transformed array with transformed values\n",
    "# Convert the array back to a dataframe and check scaled result\n",
    "cars_scaled_df = pd.DataFrame(cars_scaled)\n",
    "cars_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original hp and weight columns from original cars dataframe and concatenate it with scaled columns\n",
    "cars_dropped = cars.drop(col_scale, axis=1)\n",
    "cars_preprocessed = pd.concat([cars_scaled_df, cars_dropped], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51938124",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d760f6",
   "metadata": {},
   "source": [
    "### Data normalization \n",
    "\n",
    "Normalizing the data means to rescale it from the original range so that all values lie within the new range of 0 and 1.\n",
    "We can easily do this by using the [Min-Max-Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from sklearn. This scaler transforms the feature(s) by scaling it(them) to a given range (default range is 0 to 1). \n",
    "\n",
    "A value is normalized as follows: \n",
    "\n",
    "$ x_{scaled} = \\frac{x – x_{min}}{x_{max} – x_{min}} $\n",
    "\n",
    "(Where the min and max values pertain to the value x being normalized, from your **train** dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac67176",
   "metadata": {},
   "source": [
    "#### Hands-On now\n",
    "Try out doing normalizing and use the MinMaxScaler() for normalizing weight and horsepower as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0bf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with MinMaxScaler\n",
    "\n",
    "# Try to scale you data with the MinMaxScaler() from sklearn. \n",
    "# It follows the same syntax as the StandardScaler.\n",
    "# Don't forget: you have to import the scaler at the top of your notebook. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9196ed29717c032a5647b3d4e686475ff2d81ecd1a2edd58fd17f587842f5f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
